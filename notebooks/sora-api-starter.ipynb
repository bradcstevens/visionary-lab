{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create Videos with the Azure OpenAI Sora API\n",
        "\n",
        "This notebook demonstrates how to generate and manage videos using the Sora REST API. With Sora, you can:\n",
        "\n",
        "- **Create engaging videos** from text prompts (text-to-video)\n",
        "- **Generate videos from images** using reference images (image-to-video)\n",
        "- **Automatic audio generation** - all videos include synchronized audio\n",
        "- **Retrieve and track job statuses** to monitor your video generation progress\n",
        "- **Download videos** to your local environment\n",
        "- **Manage** your video generation jobs seamlessly\n",
        "\n",
        "In addition to Sora, we are using the Azure OpenAI GPT-4.1 model to analyze the content of generated videos in this demo.\n",
        "\n",
        "### Sora Specifications\n",
        "- **Resolutions:** 1280x720 (landscape), 720x1280 (portrait)\n",
        "- **Duration:** 4, 8, or 12 seconds\n",
        "- **Concurrency:** max 2 pending jobs\n",
        "- **Jobs expire:** after 24 hours\n",
        "\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìñ Prompt Engineering Guide\n",
        "\n",
        "Effective prompts are crucial for generating high-quality videos with Sora. According to the [Azure OpenAI documentation](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/video-generation), a well-structured prompt should include:\n",
        "\n",
        "| Element | Description | Example |\n",
        "|---------|-------------|---------|\n",
        "| **Subject** | The main focus of the video | \"A sleek silver sports car\" |\n",
        "| **Action** | What the subject is doing | \"accelerates down a highway\" |\n",
        "| **Setting** | Environment or background | \"through a neon-lit cyberpunk city at night\" |\n",
        "| **Camera Details** | Angles, movements, shot types | \"tracking shot following from the side\" |\n",
        "| **Lighting & Mood** | Ambiance and lighting conditions | \"dramatic headlights cutting through rain\" |\n",
        "\n",
        "### Example Prompt Structure\n",
        "\n",
        "```\n",
        "[Subject] + [Action] + [Setting] + [Camera/Visual Style] + [Mood/Atmosphere]\n",
        "```\n",
        "\n",
        "**Good Prompt:**\n",
        "> \"A sleek silver sports car accelerates through rain-slicked streets of a neon-lit city at night. \n",
        "> The camera follows from a low angle, capturing reflections on wet pavement. \n",
        "> Cinematic lighting with dramatic headlight beams cutting through the mist. 4K, 24fps film look.\"\n",
        "\n",
        "**Poor Prompt:**\n",
        "> \"Cool car driving fast\"\n",
        "\n",
        "### API Parameters (Set in Code, Not Prompt)\n",
        "\n",
        "These are specified in your API call, not the prompt text:\n",
        "- **Model**: `sora-2` or `sora-2-pro`\n",
        "- **Size**: `1280x720` (landscape) or `720x1280` (portrait)\n",
        "- **Seconds**: `4`, `8`, or `12`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv, find_dotenv\n",
        "from openai import AzureOpenAI\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import pandas as pd\n",
        "import threading\n",
        "from IPython.display import Video, Image, Markdown\n",
        "\n",
        "from VideoTools import Sora, VideoExtractor, VideoAnalyzer, get_video_metadata\n",
        "from Instructions import use_case_prompts, filename_system_message, analyze_video_system_message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not load_dotenv(find_dotenv()): raise IOError(\"Error: .env file could not be loaded!\")\n",
        "\n",
        "# Sora for video generation\n",
        "sora_resource_name = os.getenv(\"SORA_AOAI_RESOURCE\")\n",
        "sora_deployment_name = os.getenv(\"SORA_DEPLOYMENT\")\n",
        "sora_aoai_api_key = os.getenv(\"SORA_AOAI_API_KEY\")\n",
        "\n",
        "sora = Sora(sora_resource_name, sora_deployment_name, sora_aoai_api_key)\n",
        "\n",
        "# Azure OpenAI GPT-4.1 for analyzing videos and generating file names\n",
        "llm_deployment = os.getenv(\"LLM_DEPLOYMENT\")\n",
        "llm_aoai_api_key = os.getenv(\"LLM_AOAI_API_KEY\")\n",
        "llm_resource_name = os.getenv(\"LLM_AOAI_RESOURCE\")\n",
        "\n",
        "local_video_folder = 'video-generations'\n",
        "\n",
        "# initialize client\n",
        "client = AzureOpenAI(\n",
        "  azure_endpoint = f\"https://{llm_resource_name}.openai.azure.com/\", \n",
        "  api_key=llm_aoai_api_key,  \n",
        "  api_version=\"2025-01-01-preview\"\n",
        ")\n",
        "\n",
        "video_analyzer = VideoAnalyzer(client, llm_deployment)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following example shows how to submit a video generation request to the Sora API using `requests`. For convenience, we've provided the `Sora` class as a simple wrapper to interact easily with the API throughout the rest of this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "url = f\"https://{sora_resource_name}.openai.azure.com/openai/v1/videos\"\n",
        "\n",
        "headers = {\n",
        "    \"api-key\": sora_aoai_api_key,\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "# Sora parameters:\n",
        "# - size: \"1280x720\" (landscape) or \"720x1280\" (portrait)\n",
        "# - seconds: \"4\", \"8\", or \"12\"\n",
        "payload = {\n",
        "    \"prompt\": \"A Minecraft player exploring an ancient ruin at sunset.\",\n",
        "    \"model\": sora_deployment_name,\n",
        "    \"size\": \"1280x720\",\n",
        "    \"seconds\": \"8\"\n",
        "}\n",
        "\n",
        "response = requests.post(url, json=payload, headers=headers)\n",
        "response.raise_for_status()\n",
        "\n",
        "print(response.json())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def llm_completion(prompt, client, model, system_message):\n",
        "    \"\"\"LLM chat completion. Used for generating concise video filenames. \"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_message},\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"text\", \"text\": prompt},\n",
        "                ],\n",
        "            },\n",
        "        ],\n",
        "        response_format={\"type\": \"json_object\"},\n",
        "        temperature=0.5,\n",
        "    )\n",
        "\n",
        "    return response\n",
        "\n",
        "def check_status(job_id, polling_intervall=30):\n",
        "    \"\"\"Check video generation status until job is finished or failed.\"\"\"\n",
        "    while True:\n",
        "        \n",
        "        job = sora.get_video_generation_job(job_id=job_id)\n",
        "        status = job['status']\n",
        "        if status not in ['queued', 'preprocessing', 'running', 'processing']:\n",
        "            print(f\"\\nJob finished with status: {status}\")\n",
        "            break\n",
        "        time.sleep(polling_intervall)\n",
        "        print('.', end='', flush=True)\n",
        "\n",
        "def show_video_insights(video_path):\n",
        "    video_extractor = VideoExtractor(video_path)\n",
        "    frames = video_extractor.extract_n_video_frames(n=5)\n",
        "\n",
        "    llm_insights = video_analyzer.video_chat(frames, system_message=analyze_video_system_message)\n",
        "\n",
        "    display(Markdown(f\"Summary:  \\n{llm_insights.get('summary', '')}\"))\n",
        "    display(Markdown(f\"Products/brands: {llm_insights.get('products', '')}\"))\n",
        "    display(Markdown(f\"Tags: {llm_insights.get('tags', '')}\"))\n",
        "    display(Markdown(f\"Suggestions:  \\n{llm_insights.get('feedback', '')}\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Start Video Generation Job (Text-to-Video)\n",
        "\n",
        "Sora generates videos from text prompts with automatic audio.\n",
        "\n",
        "- **Resolutions:** 1280x720 (landscape), 720x1280 (portrait, default)\n",
        "\n",
        "- **Duration:** 4, 8, or 12 seconds (default: 4)\n",
        "\n",
        "- **Audio:** Automatically included in all generated videos\n",
        "\n",
        "- **Concurrency:** max 2 pending jobs\n",
        "\n",
        "- **Content restrictions:** No copyrighted content, no real people, no faces in input images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = use_case_prompts['Coolstuff']['Cyberpunk eye reflection']\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Available sizes: 1280x720 (landscape) or 720x1280 (portrait)\n",
        "# Duration: 4, 8, or 12 seconds\n",
        "job = sora.create_video_generation_job(\n",
        "    prompt=prompt,\n",
        "    n_seconds=8,\n",
        "    width=1280,\n",
        "    height=720\n",
        ")\n",
        "\n",
        "job_id = job['id']\n",
        "print(f\"Created job: {job_id}\")\n",
        "print(f\"Video will be {job['n_seconds']}s at {job['width']}x{job['height']}\")\n",
        "\n",
        "# Poll status as background process\n",
        "threading.Thread(target=check_status, args=(job_id,), daemon=True).start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download and Analyze Videos\n",
        "In addition to the Sora video generation model, we use **GPT-4.1** to:\n",
        "\n",
        "1. Generate a concise, descriptive filename based on the original text prompt.\n",
        "2. Analyze the video content to: Summarize key scenes, identify visible brands, and provide suggestions for improvement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If you want to retrieve a specific job\n",
        "# job_id = \"video_68ff672709d481908f1fa7c53265d835\"\n",
        "\n",
        "job = sora.get_video_generation_job(job_id=job_id)\n",
        "print(f\"Job status: {job['status']}\")\n",
        "\n",
        "downloaded_videos = []\n",
        "\n",
        "if job['status'] == 'succeeded' and job['generations']:\n",
        "    # Generate filename using LLM\n",
        "    prompt_text = job['prompt']\n",
        "    result = llm_completion(prompt_text, client, llm_deployment, filename_system_message)\n",
        "    scene_summary = json.loads(result.choices[0].message.content)['filename']\n",
        "\n",
        "    video_id = job['generations'][0]['id']\n",
        "    filename = f\"{scene_summary}_{video_id}.mp4\"\n",
        "\n",
        "    _ = sora.get_video_generation_video_content(\n",
        "        generation_id=video_id, \n",
        "        file_name=filename, \n",
        "        target_folder=local_video_folder\n",
        "    )\n",
        "    print(f'Downloaded {filename} to folder: {local_video_folder}')\n",
        "    downloaded_videos.append(filename)\n",
        "else:\n",
        "    print(f\"Job not completed or failed: {job.get('failure_reason', 'N/A')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display and analyze downloaded videos\n",
        "for videofile in downloaded_videos:\n",
        "    video_path = os.path.join(local_video_folder, videofile)\n",
        "    print(f'Video: {videofile}')\n",
        "    display(Video(video_path))\n",
        "    show_video_insights(video_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üñºÔ∏è Image-to-Video Generation\n",
        "\n",
        "Sora supports using a reference image as a **visual anchor for the first frame**. This is useful for:\n",
        "- Animating product shots\n",
        "- Creating videos that match existing brand imagery\n",
        "- Bringing still images to life\n",
        "\n",
        "According to the [Azure OpenAI docs](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/video-generation):\n",
        "- **Input Reference**: A single image used as visual anchor for the first frame\n",
        "- **Supported formats**: `image/jpeg`, `image/png`, `image/webp`\n",
        "- **Requirement**: Image should match the target video size for best results\n",
        "- **Restriction**: Images with human faces are not supported\n",
        "\n",
        "### API Implementation\n",
        "\n",
        "The `input_reference` parameter is sent via **multipart form data** (not JSON):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Image-to-Video: Animate a reference image\n",
        "# Using the briefcase.png as the visual anchor for the first frame\n",
        "\n",
        "reference_image_path = \"images/briefcase.png\"\n",
        "\n",
        "# Display the reference image\n",
        "display(Markdown(\"### Reference Image\"))\n",
        "display(Image(filename=reference_image_path, width=400))\n",
        "\n",
        "# Prompt describes the motion/action to apply to the image\n",
        "image_to_video_prompt = \"\"\"\n",
        "A professional dark laptop bag on a clean white background. The camera slowly rotates \n",
        "around the bag, showcasing its premium leather texture, metal hardware details, and \n",
        "functional compartments. Soft studio lighting highlights the craftsmanship. \n",
        "The scene is elegant and minimalist, perfect for an e-commerce product showcase.\n",
        "Smooth 360-degree rotation. 4K quality, professional product photography style.\n",
        "\"\"\"\n",
        "\n",
        "print(f\"Prompt: {image_to_video_prompt.strip()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Image-to-Video generation job\n",
        "# The Sora wrapper handles multipart form data for input_reference automatically\n",
        "\n",
        "image_job = sora.create_video_generation_job_with_image(\n",
        "    prompt=image_to_video_prompt,\n",
        "    image_path=reference_image_path,\n",
        "    n_seconds=8,\n",
        "    width=1280,\n",
        "    height=720  # Landscape orientation\n",
        ")\n",
        "\n",
        "image_job_id = image_job['id']\n",
        "print(f\"Created image-to-video job: {image_job_id}\")\n",
        "print(f\"Video will be {image_job['n_seconds']}s at {image_job['width']}x{image_job['height']}\")\n",
        "\n",
        "# Poll status in background\n",
        "threading.Thread(target=check_status, args=(image_job_id,), daemon=True).start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download and display the image-to-video result\n",
        "# image_job_id = \"video_xxx\"  # Uncomment to retrieve a specific job\n",
        "\n",
        "image_job = sora.get_video_generation_job(job_id=image_job_id)\n",
        "print(f\"Job status: {image_job['status']}\")\n",
        "\n",
        "if image_job['status'] == 'succeeded' and image_job['generations']:\n",
        "    # Generate descriptive filename\n",
        "    result = llm_completion(image_to_video_prompt, client, llm_deployment, filename_system_message)\n",
        "    scene_summary = json.loads(result.choices[0].message.content)['filename']\n",
        "    \n",
        "    video_id = image_job['generations'][0]['id']\n",
        "    img2vid_filename = f\"img2vid_{scene_summary}_{video_id}.mp4\"\n",
        "    \n",
        "    img2vid_path = sora.get_video_generation_video_content(\n",
        "        generation_id=video_id,\n",
        "        file_name=img2vid_filename,\n",
        "        target_folder=local_video_folder\n",
        "    )\n",
        "    \n",
        "    print(f\"Downloaded: {img2vid_filename}\")\n",
        "    display(Markdown(\"### Generated Video from Image\"))\n",
        "    display(Video(img2vid_path))\n",
        "    \n",
        "    # Show video insights using VideoAnalyzer\n",
        "    show_video_insights(img2vid_path)\n",
        "else:\n",
        "    print(f\"Job not completed: {image_job.get('failure_reason', 'Still processing...')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Advanced Video Analysis with VideoTools\n",
        "\n",
        "The `VideoTools.py` module provides utilities for deeper video analysis:\n",
        "\n",
        "| Class/Function | Purpose |\n",
        "|----------------|---------|\n",
        "| `VideoExtractor` | Extract frames at intervals or N equally-spaced frames |\n",
        "| `VideoAnalyzer` | Send frames to GPT-4 vision for multimodal analysis |\n",
        "| `get_video_metadata()` | Get duration, fps, resolution, bitrate |\n",
        "\n",
        "### Use Cases\n",
        "1. **Quality Assessment** - Compare prompt intent vs generated output\n",
        "2. **Content Moderation** - Detect brands, products, inappropriate content\n",
        "3. **Metadata Generation** - Auto-generate tags for video libraries\n",
        "4. **A/B Testing** - Compare multiple generations systematically"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced Analysis: Compare prompt intent vs generated output\n",
        "# This helps evaluate how well Sora interpreted your prompt\n",
        "\n",
        "quality_assessment_prompt = \"\"\"You are a video quality assessor for AI-generated content.\n",
        "Analyze the video frames and evaluate:\n",
        "\n",
        "1. **Prompt Adherence** (1-10): How well does the video match the original prompt?\n",
        "2. **Visual Quality** (1-10): Clarity, consistency, absence of artifacts\n",
        "3. **Motion Quality** (1-10): Smooth, natural movement without glitches\n",
        "4. **Composition** (1-10): Camera work, framing, visual appeal\n",
        "\n",
        "Also identify:\n",
        "- What was captured well\n",
        "- What was missed or incorrect\n",
        "- Suggestions for prompt improvement\n",
        "\n",
        "Return as JSON:\n",
        "{\n",
        "    \"scores\": {\n",
        "        \"prompt_adherence\": <1-10>,\n",
        "        \"visual_quality\": <1-10>,\n",
        "        \"motion_quality\": <1-10>,\n",
        "        \"composition\": <1-10>,\n",
        "        \"overall\": <1-10>\n",
        "    },\n",
        "    \"captured_well\": \"<what worked>\",\n",
        "    \"missed\": \"<what was missed or incorrect>\",\n",
        "    \"prompt_suggestions\": \"<how to improve the prompt>\"\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "def assess_video_quality(video_path: str, original_prompt: str) -> dict:\n",
        "    \"\"\"Comprehensive quality assessment comparing video to original prompt.\"\"\"\n",
        "    extractor = VideoExtractor(video_path)\n",
        "    frames = extractor.extract_n_video_frames(n=8)  # More frames for thorough analysis\n",
        "    \n",
        "    # Include original prompt context for the assessor\n",
        "    assessment_context = f\"Original prompt used to generate this video:\\n{original_prompt}\\n\\nNow analyze the video:\"\n",
        "    \n",
        "    # Build message with prompt context\n",
        "    content_segments = [{\"type\": \"text\", \"text\": assessment_context}]\n",
        "    for f in frames:\n",
        "        content_segments.append({\n",
        "            \"type\": \"image_url\",\n",
        "            \"image_url\": {\"url\": f\"data:image/jpg;base64,{f['frame_base64']}\", \"detail\": \"auto\"}\n",
        "        })\n",
        "        content_segments.append({\"type\": \"text\", \"text\": f\"timestamp: {f['timestamp']}\"})\n",
        "    \n",
        "    response = client.chat.completions.create(\n",
        "        model=llm_deployment,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": quality_assessment_prompt},\n",
        "            {\"role\": \"user\", \"content\": content_segments}\n",
        "        ],\n",
        "        temperature=0,\n",
        "        response_format={\"type\": \"json_object\"}\n",
        "    )\n",
        "    \n",
        "    return json.loads(response.choices[0].message.content)\n",
        "\n",
        "print(\"‚úÖ Quality assessment function ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Collect all videos we've created in this session for quality assessment\n",
        "\n",
        "videos_to_assess = []\n",
        "\n",
        "# Add text-to-video result if available\n",
        "if 'downloaded_videos' in dir() and downloaded_videos:\n",
        "    for vf in downloaded_videos:\n",
        "        videos_to_assess.append({\n",
        "            \"path\": os.path.join(local_video_folder, vf),\n",
        "            \"prompt\": prompt,\n",
        "            \"type\": \"Text-to-Video\"\n",
        "        })\n",
        "\n",
        "# Add image-to-video result if available  \n",
        "if 'img2vid_path' in dir():\n",
        "    videos_to_assess.append({\n",
        "        \"path\": img2vid_path,\n",
        "        \"prompt\": image_to_video_prompt,\n",
        "        \"type\": \"Image-to-Video\"\n",
        "    })\n",
        "\n",
        "print(f\"Found {len(videos_to_assess)} videos to assess\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run comprehensive quality assessment on all videos\n",
        "assessment_results = []\n",
        "\n",
        "for video_info in videos_to_assess:\n",
        "    video_path = video_info[\"path\"]\n",
        "    if not os.path.exists(video_path):\n",
        "        print(f\"‚ö†Ô∏è Skipping {video_path} - file not found\")\n",
        "        continue\n",
        "        \n",
        "    display(Markdown(f\"### Assessing: {video_info['type']}\"))\n",
        "    display(Markdown(f\"**File:** `{os.path.basename(video_path)}`\"))\n",
        "    \n",
        "    # Get metadata\n",
        "    metadata = get_video_metadata(video_path)\n",
        "    display(Markdown(f\"**Metadata:** {metadata['duration']}s, {metadata['resolution']}, {metadata['fps']} fps\"))\n",
        "    \n",
        "    # Run quality assessment\n",
        "    assessment = assess_video_quality(video_path, video_info[\"prompt\"])\n",
        "    assessment_results.append({\n",
        "        \"type\": video_info[\"type\"],\n",
        "        \"file\": os.path.basename(video_path),\n",
        "        **assessment.get(\"scores\", {})\n",
        "    })\n",
        "    \n",
        "    # Display scores\n",
        "    scores = assessment.get(\"scores\", {})\n",
        "    display(Markdown(f\"\"\"\n",
        "| Metric | Score |\n",
        "|--------|-------|\n",
        "| Prompt Adherence | {scores.get('prompt_adherence', 'N/A')}/10 |\n",
        "| Visual Quality | {scores.get('visual_quality', 'N/A')}/10 |\n",
        "| Motion Quality | {scores.get('motion_quality', 'N/A')}/10 |\n",
        "| Composition | {scores.get('composition', 'N/A')}/10 |\n",
        "| **Overall** | **{scores.get('overall', 'N/A')}/10** |\n",
        "\"\"\"))\n",
        "    \n",
        "    display(Markdown(f\"**‚úÖ Captured Well:** {assessment.get('captured_well', 'N/A')}\"))\n",
        "    display(Markdown(f\"**‚ùå Missed:** {assessment.get('missed', 'N/A')}\"))\n",
        "    display(Markdown(f\"**üí° Prompt Suggestions:** {assessment.get('prompt_suggestions', 'N/A')}\"))\n",
        "    display(Markdown(\"---\"))\n",
        "\n",
        "# Summary DataFrame\n",
        "if assessment_results:\n",
        "    display(Markdown(\"### üìà Assessment Summary\"))\n",
        "    summary_df = pd.DataFrame(assessment_results)\n",
        "    display(summary_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Manage Video Generation Jobs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List all jobs \n",
        "jobs = sora.list_video_generation_jobs(limit=50)\n",
        "\n",
        "df = pd.DataFrame(jobs['data'])\n",
        "if not df.empty:\n",
        "    # Calculate duration for completed jobs\n",
        "    df['duration_s'] = df.apply(\n",
        "        lambda row: (row['finished_at'] - row['created_at']) \n",
        "        if row['finished_at'] and row['created_at'] else None, \n",
        "        axis=1\n",
        "    )\n",
        "    columns = ['id', 'status', 'created_at', 'finished_at', 'duration_s', \n",
        "               'prompt', 'n_seconds', 'height', 'width', 'has_audio', 'failure_reason']\n",
        "    available_columns = [c for c in columns if c in df.columns]\n",
        "    df = df[available_columns]\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# retrieve a specific job\n",
        "\n",
        "job_id = df.iloc[1]['id']\n",
        "job = sora.get_video_generation_job(job_id=job_id)\n",
        "display(job)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract a frame from a video as a thumbnail/preview\n",
        "if job['generations']:\n",
        "    video_id = job['generations'][0]['id']\n",
        "    \n",
        "    # Download the video\n",
        "    temp_video_path = sora.get_video_generation_video_content(\n",
        "        generation_id=video_id,\n",
        "        file_name=\"temp_preview.mp4\",\n",
        "        target_folder=local_video_folder\n",
        "    )\n",
        "    \n",
        "    # Extract first frame as a preview\n",
        "    video_extractor = VideoExtractor(temp_video_path)\n",
        "    frames = video_extractor.extract_n_video_frames(n=1)\n",
        "    if frames:\n",
        "        import base64\n",
        "        from PIL import Image as PILImage\n",
        "        from io import BytesIO\n",
        "        \n",
        "        frame_data = base64.b64decode(frames[0]['frame_base64'])\n",
        "        thumbnail = PILImage.open(BytesIO(frame_data))\n",
        "        display(thumbnail)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stream a video directly into memory (without saving to disk)\n",
        "if job['generations']:\n",
        "    video_id = job['generations'][0]['id']\n",
        "\n",
        "    stream = sora.get_video_generation_video_stream(video_id)\n",
        "\n",
        "    video_bytes = stream.getvalue()\n",
        "    display(Video(data=video_bytes, embed=True, mimetype='video/mp4'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download a video from a job\n",
        "if job['generations']:\n",
        "    video_id = job['generations'][0]['id']\n",
        "\n",
        "    file_name = \"my_video.mp4\"\n",
        "    target_folder = \"video-generations/\"\n",
        "\n",
        "    file_path = sora.get_video_generation_video_content(\n",
        "        generation_id=video_id,\n",
        "        file_name=file_name,\n",
        "        target_folder=target_folder\n",
        "    ) \n",
        "\n",
        "    display(Video(file_path))\n",
        "    print(get_video_metadata(file_path))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
